{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.ones((1, 100)) * 0.5#np.random.randn(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.load(\"datasets/omniglot_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.ones((1, 28, 28, 1))\n",
    "# inp = np.array([raw_data[1200][0]], dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dagan_architectures import UResNetGenerator, Discriminator\n",
    "\n",
    "g = UResNetGenerator(batch_size=1, layer_sizes=[64, 64, 128, 128], layer_padding=None,\n",
    "                        inner_layers=[3] * 4, name=\"generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_inp = tf.placeholder(tf.float32, [1,100], 'z-inp')\n",
    "cond_inp = tf.placeholder(tf.float32, [1,28,28,1], 'cond-inp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ashwin/Workspace/DAGAN/dagan_architectures.py:209: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/ashwin/Workspace/DAGAN/dagan_architectures.py:83: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /Library/Python/3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/ashwin/Workspace/DAGAN/dagan_architectures.py:127: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /Users/ashwin/Workspace/DAGAN/dagan_architectures.py:271: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "Tensor(\"generator/vector_expansion/Reshape:0\", shape=(1, 2, 2, 8), dtype=float32)\n",
      "Tensor(\"generator/vector_expansion/Reshape_1:0\", shape=(1, 4, 4, 4), dtype=float32)\n",
      "Tensor(\"generator/vector_expansion/Reshape_2:0\", shape=(1, 7, 7, 2), dtype=float32)\n",
      "Tensor(\"generator/vector_expansion/Reshape_3:0\", shape=(1, 14, 14, 1), dtype=float32)\n",
      "WARNING:tensorflow:From /Users/ashwin/Workspace/DAGAN/dagan_architectures.py:59: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/ashwin/Workspace/DAGAN/dagan_architectures.py:80: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
      "WARNING:tensorflow:From /Users/ashwin/Workspace/DAGAN/dagan_architectures.py:361: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/ashwin/Workspace/DAGAN/dagan_architectures.py:361: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "generator_total_layers 50\n",
      "generator_parameter_num 10546217\n"
     ]
    }
   ],
   "source": [
    "result = g(z_inp, cond_inp, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/checkpoint.ckpt\"\n",
    "fine_tune = slim.assign_from_checkpoint_fn(\n",
    "    checkpoint,\n",
    "    tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "\n",
    "sess = tf.Session()\n",
    "fine_tune(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4691119"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = sess.run(\n",
    "    result,\n",
    "    feed_dict={z_inp: z, cond_inp: inp},\n",
    ")\n",
    "final_results[0][0, :, :, 0][5][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv2d(in_channels, out_channels, kernel_size, stride, activate=True, dropout=0.0):\n",
    "    layers = OrderedDict()\n",
    "    if stride == 2:\n",
    "        padding = 0\n",
    "        layers['pad'] = nn.ZeroPad2d((0, 1, 0, 1))\n",
    "    else:\n",
    "        padding = 1\n",
    "    layers['conv'] = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "    if activate:\n",
    "        layers['relu'] = nn.LeakyReLU(0.2)\n",
    "        layers['batchnorm'] = nn.BatchNorm2d(out_channels, eps=1e-3, momentum=0.01)\n",
    "    \n",
    "    if dropout > 0.0:\n",
    "        layers['dropout'] = nn.Dropout(dropout)\n",
    "    return nn.Sequential(layers)\n",
    "\n",
    "\n",
    "class _EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, pre_channels, in_channels, out_channels, num_layers, dropout_rate=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.pre_conv = _conv2d(\n",
    "            in_channels=pre_channels, out_channels=pre_channels, kernel_size=3, stride=2, activate=False\n",
    "        )\n",
    "\n",
    "        self.conv0 = _conv2d(\n",
    "            in_channels=in_channels + pre_channels, out_channels=out_channels, kernel_size=3, stride=1\n",
    "        )\n",
    "        total_channels = in_channels + out_channels\n",
    "        for i in range(1, num_layers):\n",
    "            self.add_module(\n",
    "                'conv%d' % i,\n",
    "                _conv2d(\n",
    "                    in_channels=total_channels, out_channels=out_channels, kernel_size=3, stride=1\n",
    "                )\n",
    "            )\n",
    "            total_channels += out_channels\n",
    "        self.add_module(\n",
    "            'conv%d' % num_layers,\n",
    "            _conv2d(\n",
    "                in_channels=total_channels, out_channels=out_channels, kernel_size=3, stride=2, dropout=dropout_rate\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        pre_input, x = inp\n",
    "        if pre_input.shape[2] % 2 == 1:\n",
    "            pre_input = F.pad(pre_input, (1, 0, 1, 0))\n",
    "        pre_input = self.pre_conv(pre_input)\n",
    "        out = self.conv0(torch.cat([x, pre_input], 1))\n",
    "#         print(out[0][0])\n",
    "        \n",
    "        all_outputs = [x, out]\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            input_features = torch.cat([all_outputs[-1], all_outputs[-2]] + all_outputs[:-2], 1)\n",
    "            module = self._modules['conv%d' % i]\n",
    "            if input_features.shape[2] % 2 == 1 and module.conv.stride[0] == 2:\n",
    "                input_features = F.pad(input_features, (1, 0, 1, 0))\n",
    "            out = module(input_features)\n",
    "            all_outputs.append(out)\n",
    "#         print([o[0][0] for o in all_outputs])\n",
    "        return all_outputs[-2], all_outputs[-1]\n",
    "\n",
    "\n",
    "class TorchGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_sizes = [64, 64, 128, 128]\n",
    "        self.num_inner_layers = 3\n",
    "        self.conv0 = _conv2d(\n",
    "            in_channels=1, out_channels=self.layer_sizes[0], kernel_size=3, stride=2\n",
    "        )\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            self.add_module(\n",
    "                \"encode%d\" % i,\n",
    "                _EncoderBlock(\n",
    "                    self.layer_sizes[i - 1], self.layer_sizes[i - 1], self.layer_sizes[i], self.num_inner_layers\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = [x, self.conv0(x)]\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            out = self._modules[\"encode%d\" % i](out)\n",
    "            print(out[0].shape, out[1].shape)\n",
    "#             if i == 1:\n",
    "#                 print(out[1][0][0])\n",
    "#         out = self.encode1(out)\n",
    "#         out = self.act(out)\n",
    "#         out = self.bn(out)\n",
    "        return out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchGAN(\n",
       "  (conv0): Sequential(\n",
       "    (pad): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "    (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (relu): LeakyReLU(negative_slope=0.2)\n",
       "    (batchnorm): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (encode1): _EncoderBlock(\n",
       "    (pre_conv): Sequential(\n",
       "      (pad): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (conv0): Sequential(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): Sequential(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (pad): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (encode2): _EncoderBlock(\n",
       "    (pre_conv): Sequential(\n",
       "      (pad): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (conv0): Sequential(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): Sequential(\n",
       "      (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (conv): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (pad): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      (conv): Conv2d(448, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (encode3): _EncoderBlock(\n",
       "    (pre_conv): Sequential(\n",
       "      (pad): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (conv0): Sequential(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): Sequential(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (conv): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (pad): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (relu): LeakyReLU(negative_slope=0.2)\n",
       "      (batchnorm): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_g = TorchGAN()\n",
    "torch_g.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_reader = tf.train.load_checkpoint(checkpoint)\n",
    "def read_tf(var):\n",
    "    return torch.tensor(ckpt_reader.get_tensor(var))\n",
    "\n",
    "def reshape_conv(tensor):\n",
    "    return tensor.transpose(0, 3).transpose(1, 2).transpose(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conv_layer(torch_layer, conv_path, bn_path):\n",
    "    torch_layer.batchnorm.weight.data = read_tf(bn_path + \"/gamma\")\n",
    "    torch_layer.batchnorm.bias.data = read_tf(bn_path + \"/beta\")\n",
    "    torch_layer.batchnorm.running_mean = read_tf(bn_path + \"/moving_mean\")\n",
    "    torch_layer.batchnorm.running_var = read_tf(bn_path + \"/moving_variance\")\n",
    "    \n",
    "    torch_layer.conv.weight.data = reshape_conv(read_tf(conv_path + \"/kernel\"))\n",
    "    torch_layer.conv.bias.data = read_tf(conv_path + \"/bias\")\n",
    "\n",
    "convert_conv_layer(\n",
    "    torch_g.conv0, \"generator/conv_layers/g_conv0/conv2d\", \"generator/conv_layers/g_conv0/BatchNorm\")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    module = torch_g._modules[\"encode%d\" % i]\n",
    "    module.pre_conv.conv.weight.data = reshape_conv(read_tf(f\"generator/conv_layers/g_conv{i}/conv2d/kernel\"))\n",
    "    module.pre_conv.conv.bias.data = read_tf(f\"generator/conv_layers/g_conv{i}/conv2d/bias\")\n",
    "\n",
    "    for j in range(4):\n",
    "        bn_suffix = f\"_{j}\" if j > 0 else \"\"\n",
    "        convert_conv_layer(\n",
    "            module._modules[f\"conv{j}\"],\n",
    "            f\"generator/conv_layers/g_conv{i}/conv2d_{j + 1}\",\n",
    "            f\"generator/conv_layers/g_conv{i}/BatchNorm{bn_suffix}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 14, 14]) torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 128, 7, 7]) torch.Size([1, 128, 4, 4])\n",
      "torch.Size([1, 128, 4, 4]) torch.Size([1, 128, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5380, -0.3327],\n",
       "        [ 0.2860, -0.3094]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum(sum(torch_g.conv.weight[0][0]))\n",
    "# print(torch_g(torch.ones(1, 1, 28, 28)).shape)\n",
    "torch_g(torch.ones(1, 1, 28, 28))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5379593 , -0.33272868],\n",
       "       [ 0.28598887, -0.30941498]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[3][13][0, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_image_arr(arr):\n",
    "    arr = np.uint8(arr * 256)\n",
    "    arr = arr.reshape(arr.shape[:-1])\n",
    "    display(Image.fromarray(arr, mode='L').resize((224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars_list = tf.train.list_variables(checkpoint)\n",
    "vars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_g.conv._modules['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_image_arr(raw_data[1200][0])\n",
    "render_image_arr(final_results[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.image.resize_nearest_neighbor(inp, (None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
